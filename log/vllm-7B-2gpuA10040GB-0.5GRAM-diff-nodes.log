INFO 09-03 04:35:48 [__init__.py:241] Automatically detected platform cuda.
(APIServer pid=121) INFO 09-03 04:36:09 [api_server.py:1805] vLLM API server version 0.10.1.1
(APIServer pid=121) INFO 09-03 04:36:09 [utils.py:326] non-default args: {'model_tag': 'Qwen2-7B-Instruct', 'port': 8081, 'model': 'Qwen
2-7B-Instruct', 'max_model_len': 16384, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.5}
(APIServer pid=121) INFO 09-03 04:36:31 [__init__.py:711] Resolved architecture: Qwen2ForCausalLM
(APIServer pid=121) INFO 09-03 04:36:31 [__init__.py:1750] Using max model len 16384
(APIServer pid=121) INFO 09-03 04:36:32 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 09-03 04:36:42 [__init__.py:241] Automatically detected platform cuda.
(EngineCore_0 pid=1257) INFO 09-03 04:36:47 [core.py:636] Waiting for init message from front-end.
(EngineCore_0 pid=1257) INFO 09-03 04:36:47 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='Qwen2-7B-Instruct'
, speculative_config=None, tokenizer='Qwen2-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={},
 tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_si
ze=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, d
ecoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_
backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None
), seed=0, served_model_name=Qwen2-7B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config
=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vl
lm.unified_attention_with_output","vllm.mamba_mixer2"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalize
d_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,4
80,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,
184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_c
onfig":{},"max_capture_size":512,"local_cache_dir":null}
(EngineCore_0 pid=1257) 2025-09-03 04:36:47,071 INFO worker.py:1747 -- Connecting to existing Ray cluster at address: 10.254.5.36:6
379...
(EngineCore_0 pid=1257) 2025-09-03 04:36:47,094 INFO worker.py:1918 -- Connected to Ray cluster. View the dashboard at 127.0
.0.1:8100 
(EngineCore_0 pid=1257) INFO 09-03 04:36:47 [ray_utils.py:339] No current placement group found. Creating a new placement group.
(EngineCore_0 pid=1257) WARNING 09-03 04:36:48 [ray_utils.py:200] tensor_parallel_size=2 is bigger than a reserved number of GPUs (1 GPU
s) in a node ad95619665703793074b79b0281a3e862204f6009bd050b8feea7495. Tensor parallel workers can be spread out to 2+ nodes which can degrade the 
performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 2 GPUs available a
t each node.
(EngineCore_0 pid=1257) WARNING 09-03 04:36:48 [ray_utils.py:200] tensor_parallel_size=2 is bigger than a reserved number of GPUs (1 GPU
s) in a node 2ac81a004da75a4b41e085ab79f35ad7b1d07565b2cc621eb8cacfc0. Tensor parallel workers can be spread out to 2+ nodes which can degrade the 
performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 2 GPUs available a
t each node.
(EngineCore_0 pid=1257) INFO 09-03 04:36:48 [ray_distributed_executor.py:169] use_ray_spmd_worker: True
(EngineCore_0 pid=1257) (pid=1384) INFO 09-03 04:36:52 [__init__.py:241] Automatically detected platform cuda.
(EngineCore_0 pid=1257) (pid=374, ip=10.254.6.219) INFO 09-03 04:38:03 [__init__.py:241] Automatically detected platform cuda.
(EngineCore_0 pid=1257) INFO 09-03 04:38:06 [ray_env.py:63] RAY_NON_CARRY_OVER_ENV_VARS from config: set()
(EngineCore_0 pid=1257) INFO 09-03 04:38:06 [ray_env.py:65] Copying the following environment variables to workers: ['VLLM_USE_V1', 'VLL
M_USE_RAY_COMPILED_DAG', 'LD_LIBRARY_PATH', 'VLLM_WORKER_MULTIPROC_METHOD', 'VLLM_USE_RAY_SPMD_WORKER']
(EngineCore_0 pid=1257) INFO 09-03 04:38:06 [ray_env.py:68] If certain env vars should NOT be copied, add them to /home/cdsw/.config/vll
m/ray_non_carry_over_env_vars.json file
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) 
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) 
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) 
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) 
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) 
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) 
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) 
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) INFO 09-03 04:38:27 [__init__.py:1418] Found nccl from library libnccl.so.2
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) INFO 09-03 04:38:27 [pynccl.py:70] vLLM is using nccl==2.26.2
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=374, ip=10.254.6.219) WARNING 09-03 04:38:28 [custom_all_reduce.py:85] Custom allre
duce is disabled because this process group spans across nodes.
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=374, ip=10.254.6.219) INFO 09-03 04:38:28 [parallel_state.py:1134] rank 1 in world 
size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) INFO 09-03 04:38:28 [shm_broadcast.py:289] vLLM message queue communication h
andle: Handle(local_reader_ranks=[], buffer_handle=None, local_subscribe_addr=None, remote_subscribe_addr='tcp://10.254.5.36:57137', remote_addr_ip
v6=False)
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=374, ip=10.254.6.219) WARNING 09-03 04:38:28 [topk_topp_sampler.py:61] FlashInfer i
s not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=374, ip=10.254.6.219) INFO 09-03 04:38:28 [gpu_model_runner.py:1953] Starting to lo
ad model Qwen2-7B-Instruct...
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=374, ip=10.254.6.219) INFO 09-03 04:38:28 [gpu_model_runner.py:1985] Loading model 
from scratch...
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=374, ip=10.254.6.219) INFO 09-03 04:38:32 [cuda.py:328] Using Flash Attention backe
nd on V1 engine.
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=374, ip=10.254.6.219) INFO 09-03 04:38:27 [__init__.py:1418] Found nccl from librar
y libnccl.so.2
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=374, ip=10.254.6.219) INFO 09-03 04:38:27 [pynccl.py:70] vLLM is using nccl==2.26.2
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=374, ip=10.254.6.219) INFO 09-03 04:41:47 [default_loader.py:262] Loading weights t
ook 194.64 seconds
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) WARNING 09-03 04:38:28 [custom_all_reduce.py:85] Custom allreduce is disabled
 because this process group spans across nodes.
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) INFO 09-03 04:38:28 [parallel_state.py:1134] rank 0 in world size 2 is assign
ed as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) WARNING 09-03 04:38:28 [topk_topp_sampler.py:61] FlashInfer is not available.
 Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) INFO 09-03 04:38:28 [gpu_model_runner.py:1953] Starting to load model Qwen2-7
B-Instruct...
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) INFO 09-03 04:38:28 [gpu_model_runner.py:1985] Loading model from scratch...
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) INFO 09-03 04:38:32 [cuda.py:328] Using Flash Attention backend on V1 engine.
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=374, ip=10.254.6.219) INFO 09-03 04:41:48 [gpu_model_runner.py:2007] Model loading 
took 7.1217 GiB and 198.627679 seconds
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) INFO 09-03 04:42:20 [backends.py:548] Using cache directory: /home/cdsw/.cach
e/vllm/torch_compile_cache/86361584b2/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) INFO 09-03 04:42:20 [backends.py:559] Dynamo bytecode transform time: 32.07 s
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) INFO 09-03 04:41:47 [default_loader.py:262] Loading weights took 194.90 secon
ds
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) INFO 09-03 04:41:48 [gpu_model_runner.py:2007] Model loading took 7.1217 GiB 
and 198.942282 seconds
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=374, ip=10.254.6.219) INFO 09-03 04:42:32 [backends.py:161] Directly load the compi
led graph(s) for dynamic shape from the cache, took 10.729 s
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=374, ip=10.254.6.219) INFO 09-03 04:42:20 [backends.py:548] Using cache directory: 
/home/cdsw/.cache/vllm/torch_compile_cache/86361584b2/rank_1_0/backbone for vLLM's torch.compile
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=374, ip=10.254.6.219) INFO 09-03 04:42:20 [backends.py:559] Dynamo bytecode transfo
rm time: 32.18 s
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=374, ip=10.254.6.219) INFO 09-03 04:42:41 [monitor.py:34] torch.compile takes 32.18
 s in total
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) INFO 09-03 04:42:33 [backends.py:161] Directly load the compiled graph(s) for
 dynamic shape from the cache, took 11.766 s
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=374, ip=10.254.6.219) INFO 09-03 04:42:46 [gpu_worker.py:276] Available KV cache me
mory: 11.03 GiB
(EngineCore_0 pid=1257) INFO 09-03 04:42:46 [kv_cache_utils.py:849] GPU KV cache size: 412,944 tokens
(EngineCore_0 pid=1257) INFO 09-03 04:42:46 [kv_cache_utils.py:853] Maximum concurrency for 16,384 tokens per request: 25.20x
(EngineCore_0 pid=1257) INFO 09-03 04:42:46 [kv_cache_utils.py:849] GPU KV cache size: 412,944 tokens
(EngineCore_0 pid=1257) INFO 09-03 04:42:46 [kv_cache_utils.py:853] Maximum concurrency for 16,384 tokens per request: 25.20x
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) 
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) 
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384)
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) 
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) INFO 09-03 04:42:58 [gpu_model_runner.py:2708] Graph capturing finished in 11
 secs, took 1.00 GiB
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) INFO 09-03 04:42:41 [monitor.py:34] torch.compile takes 32.07 s in total
(EngineCore_0 pid=1257) (RayWorkerWrapper pid=1384) INFO 09-03 04:42:46 [gpu_worker.py:276] Available KV cache memory: 11.03 GiB
(EngineCore_0 pid=1257) INFO 09-03 04:42:58 [core.py:214] init engine (profile, create kv cache, warmup model) took 69.64 seconds
(APIServer pid=121) INFO 09-03 04:42:59 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is:
 25809
(APIServer pid=121) INFO 09-03 04:43:00 [api_server.py:1611] Supported_tasks: ['generate']
(APIServer pid=121) WARNING 09-03 04:43:00 [__init__.py:1625] Default sampling parameters have been overridden by the model's Hugging Fa
ce generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
(APIServer pid=121) INFO 09-03 04:43:00 [serving_responses.py:120] Using default chat sampling params from model: {'repetition_penalty':
 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
(APIServer pid=121) INFO 09-03 04:43:00 [serving_chat.py:134] Using default chat sampling params from model: {'repetition_penalty': 1.05
, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
(APIServer pid=121) INFO 09-03 04:43:00 [serving_completion.py:77] Using default completion sampling params from model: {'repetition_pen
alty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
(APIServer pid=121) INFO 09-03 04:43:00 [api_server.py:1880] Starting vLLM API server 0 on http://0.0.0.0:8081
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:36] Available routes are:
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /openapi.json, Methods: GET, HEAD
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /docs, Methods: GET, HEAD
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /docs/oauth2-redirect, Methods: GET, HEAD
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /redoc, Methods: GET, HEAD
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /health, Methods: GET
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /load, Methods: GET
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /ping, Methods: POST
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /ping, Methods: GET
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /tokenize, Methods: POST
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /detokenize, Methods: POST
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /v1/models, Methods: GET
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /version, Methods: GET
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /v1/responses, Methods: POST
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /v1/responses/{response_id}, Methods: GET
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /v1/responses/{response_id}/cancel, Methods: POST
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /v1/chat/completions, Methods: POST
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /v1/completions, Methods: POST
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /v1/embeddings, Methods: POST
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /pooling, Methods: POST
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /classify, Methods: POST
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /score, Methods: POST
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /v1/score, Methods: POST
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /v1/audio/transcriptions, Methods: POST
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /v1/audio/translations, Methods: POST
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /rerank, Methods: POST
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /v1/rerank, Methods: POST
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /v2/rerank, Methods: POST
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /scale_elastic_ep, Methods: POST
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /is_scaling_elastic_ep, Methods: POST
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /invocations, Methods: POST
(APIServer pid=121) INFO 09-03 04:43:00 [launcher.py:44] Route: /metrics, Methods: GET
(APIServer pid=121) INFO:     Started server process [121]
(APIServer pid=121) INFO:     Waiting for application startup.
(APIServer pid=121) INFO:     Application startup complete.
